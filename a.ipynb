{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (1565229651.py, line 92)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[12], line 92\u001b[0;36m\u001b[0m\n\u001b[0;31m    obj=label_encoder)\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "from census.entity import artifact_entity,config_entity\n",
    "from census.exception import SensorException\n",
    "from census.logger import logging\n",
    "from typing import Optional\n",
    "import os,sys \n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "from census import utils\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from census.config import TARGET_COLUMN\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self,data_transformation_config:config_entity.DataTransformationConfig\n",
    "                 ,data_ingestion_artifact= artifact_entity.DataIngestionArtifact):\n",
    "        try:\n",
    "            logging.info(f\"{'>>'*20} Data Transformation {'<<'*20}\")\n",
    "            self.data_transformation_config=data_transformation_config\n",
    "            self.data_ingestion_artifact=data_ingestion_artifact\n",
    "        except Exception as e :\n",
    "            raise SensorException(e, sys)\n",
    "\n",
    "    \n",
    "    @classmethod\n",
    "    def get_data_transformer_object(cls)->Pipeline:\n",
    "        try:\n",
    "            simple_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "            robust_scaler =  RobustScaler()\n",
    "            pipeline = Pipeline(steps=[\n",
    "                    ('Imputer',simple_imputer),\n",
    "                    ('RobustScaler',robust_scaler)\n",
    "                ])\n",
    "            return pipeline\n",
    "        except Exception as e:\n",
    "            raise SensorException(e, sys)\n",
    "\n",
    "    def initiate_data_transformation(self,)->artifact_entity.DataTransformationArtifact:\n",
    "        try: \n",
    "            #reading training and testing file\n",
    "            train_df = pd.read_csv(self.data_ingestion_artifact.train_file_path)\n",
    "            test_df = pd.read_csv(self.data_ingestion_artifact.test_file_path)\n",
    "            #selecting input feature for train and test file\n",
    "            input_feature_train_df = train_df.drop(TARGET_COLUMN)\n",
    "            input_feature_test_df = test_df.drop(TARGET_COLUMN)\n",
    "            #selecting target feature for train and test file\n",
    "            target_feature_train_df = train_df[TARGET_COLUMN]\n",
    "            target_feature_test_df = test_df[TARGET_COLUMN]\n",
    "            #using label encoder for transformation\n",
    "            label_encoder = LabelEncoder()\n",
    "            label_encoder.fit(target_feature_train_df)\n",
    "\n",
    "            #transformation on target columns\n",
    "            target_feature_train_arr = label_encoder.transform(target_feature_train_df)\n",
    "            target_feature_test_arr = label_encoder.transform(target_feature_test_df)\n",
    "\n",
    "            transformation_pipleine = DataTransformation.get_data_transformer_object()\n",
    "            transformation_pipleine.fit(input_feature_train_df)\n",
    "\n",
    "            input_feature_train_arr = label_encoder.transform(input_feature_train_df)\n",
    "            input_feature_test_arr = label_encoder.transform(input_feature_test_df)\n",
    "\n",
    "\n",
    "            smt = SMOTETomek(random_state=42)#\n",
    "            logging.info(f\"Before resampling in training set Input: {input_feature_train_arr.shape} Target:{target_feature_train_arr.shape}\")\n",
    "            input_feature_train_arr, target_feature_train_arr = smt.fit_resample(input_feature_train_arr, target_feature_train_arr)\n",
    "            logging.info(f\"After resampling in training set Input: {input_feature_train_arr.shape} Target:{target_feature_train_arr.shape}\")\n",
    "            \n",
    "            logging.info(f\"Before resampling in testing set Input: {input_feature_test_arr.shape} Target:{target_feature_test_arr.shape}\")\n",
    "            input_feature_test_arr, target_feature_test_arr = smt.fit_resample(input_feature_test_arr, target_feature_test_arr)\n",
    "            logging.info(f\"After resampling in testing set Input: {input_feature_test_arr.shape} Target:{target_feature_test_arr.shape}\")\n",
    "\n",
    "\n",
    "            #target encoder\n",
    "            train_arr = np.c_[input_feature_train_arr, target_feature_train_arr ]\n",
    "            test_arr = np.c_[input_feature_test_arr, target_feature_test_arr]\n",
    "\n",
    "            #save numpy array\n",
    "            utils.save_numpy_array_data(file_path=self.data_transformation_config.transformed_train_path,\n",
    "                                        array=train_arr)\n",
    "\n",
    "            utils.save_numpy_array_data(file_path=self.data_transformation_config.transformed_test_path,\n",
    "                                        array=test_arr)\n",
    "\n",
    "\n",
    "            utils.save_object(file_path=self.data_transformation_config.transform_object_path,\n",
    "             obj=transformation_pipleine)\n",
    "\n",
    "            utils.save_object(file_path=self.data_transformation_config.target_encoder_path,\n",
    "            obj=label_encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m utils\u001b[38;5;241m.\u001b[39msave_object(file_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241m.\u001b[39mdata_transformation_config\u001b[38;5;241m.\u001b[39mtransform_object_path,\n\u001b[1;32m      2\u001b[0m  obj\u001b[38;5;241m=\u001b[39mtransformation_pipleine)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "            utils.save_object(file_path=self.data_transformation_config.transform_object_path,\n",
    "             obj=transformation_pipleine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading environment variable from .env file\n"
     ]
    }
   ],
   "source": [
    "from census import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m input_feature_train_df \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_df\u001b[49m\u001b[38;5;241m.\u001b[39mdrop(TARGET_COLUMN)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "input_feature_train_df = train_df.drop(TARGET_COLUMN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241m.\u001b[39mdata_ingestion_artifact\u001b[38;5;241m.\u001b[39mtrain_file_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(self.data_ingestion_artifact.train_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'artifact_entity' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDataTransformation\u001b[39;00m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,data_transformation_config:config_entity\u001b[38;5;241m.\u001b[39mDataTransformationConfig\n\u001b[1;32m      3\u001b[0m                  ,data_ingestion_artifact\u001b[38;5;241m=\u001b[39m artifact_entity\u001b[38;5;241m.\u001b[39mDataIngestionArtifact):\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m, in \u001b[0;36mDataTransformation\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDataTransformation\u001b[39;00m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,data_transformation_config:config_entity\u001b[38;5;241m.\u001b[39mDataTransformationConfig\n\u001b[0;32m----> 3\u001b[0m                  ,data_ingestion_artifact\u001b[38;5;241m=\u001b[39m \u001b[43martifact_entity\u001b[49m\u001b[38;5;241m.\u001b[39mDataIngestionArtifact):\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m             logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m20\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Data Transformation \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<<\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m20\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'artifact_entity' is not defined"
     ]
    }
   ],
   "source": [
    "class DataTransformation:\n",
    "    def __init__(self,data_transformation_config:config_entity.DataTransformationConfig\n",
    "                 ,data_ingestion_artifact= artifact_entity.DataIngestionArtifact):\n",
    "        try:\n",
    "            logging.info(f\"{'>>'*20} Data Transformation {'<<'*20}\")\n",
    "            self.data_transformation_config=data_transformation_config\n",
    "            self.data_ingestion_artifact=data_ingestion_artifact\n",
    "        except Exception as e :\n",
    "            raise SensorException(e, sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#reading training and testing file\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241m.\u001b[39mdata_ingestion_artifact\u001b[38;5;241m.\u001b[39mtrain_file_path)\n\u001b[1;32m      3\u001b[0m test_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_ingestion_artifact\u001b[38;5;241m.\u001b[39mtest_file_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "            #reading training and testing file\n",
    "            train_df = pd.read_csv(self.data_ingestion_artifact.train_file_path)\n",
    "            test_df = pd.read_csv(self.data_ingestion_artifact.test_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            1.\n",
    "            label_encoder = LabelEncoder()\n",
    "            label_encoder.fit(target_feature_train_df)\n",
    "\n",
    "            #transformation on target columns\n",
    "            target_feature_train_arr = label_encoder.transform(target_feature_train_df)\n",
    "            target_feature_test_arr = label_encoder.transform(target_feature_test_df)\n",
    "\n",
    "            transformation_pipleine = DataTransformation.get_data_transformer_object()\n",
    "            transformation_pipleine.fit(input_feature_train_df)\n",
    "            \n",
    "            input_feature_train_arr = label_encoder.transform(input_feature_train_df)\n",
    "            input_feature_test_arr = label_encoder.transform(input_feature_test_df)\n",
    "\n",
    "                        label_encoder2 = LabelEncoder()\n",
    "            2.\n",
    "            label_encoder2 = LabelEncoder()\n",
    "            #transformation on target columns\n",
    "            target_feature_train_arr = label_encoder2.fit_transform(target_feature_train_df)\n",
    "            target_feature_test_arr = label_encoder2.transform(target_feature_test_df)\n",
    "\n",
    "            transformation_pipleine = DataTransformation.get_data_transformer_object()\n",
    "            transformation_pipleine.fit(input_feature_train_df)\n",
    "\n",
    "            #transforming input features\n",
    "            input_feature_train_arr = transformation_pipleine.transform(input_feature_train_df)\n",
    "            input_feature_test_arr = transformation_pipleine.transform(input_feature_test_df)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            data_transformation_artifact = artifact_entity.DataTransformationArtifact(\n",
    "                transform_object_path = self.data_transformation_artifact.transform_object_path\n",
    "                ,  transformed_train_path= self.data_transformation_config.transformed_train_path\n",
    "                , transformed_test_path= self.data_transformation_config.transformed_test_path\n",
    "                , target_encoder_path= self.data_transformation_config.target_encoder_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
